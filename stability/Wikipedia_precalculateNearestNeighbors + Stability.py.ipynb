{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d211d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precalculate the five nearest neighbors for every word for every language in Wikipedia.\n",
    "\n",
    "import faiss\n",
    "import time\n",
    "import tables as tb\n",
    "import pickle\n",
    "from sklearn.neighbors import BallTree\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from tqdm import tqdm,trange\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# SET THESE VARIABLES\n",
    "\n",
    "# Location of the GloVe embedding spaces (created using trainGlove_wikipedia.sh)\n",
    "# Files should be stored in the format {glove_path}{language}/downsampled_without_replacement_glove_{downsample_index}\n",
    "glove_path = '/Users/nehakardam/Project-CSE517/polyglot/'\n",
    "\n",
    "# Location where output nearest neighbors will be stored\n",
    "# Files will be stored in the format {output_path}{language}/downsampled_without_replacement_glove_nearestNeighbors_{downsample_index}.pkl, where the pickle file is a dictionary where the keys are words and the values are lists of ten nearest neighbors for each word.\n",
    "output_path = glove_path\n",
    "\n",
    "# Indices of downsamples to process (can adjust if needed, or leave the same)\n",
    "seeds = [0,1,2,3,4]\n",
    "\n",
    "# Languages to precalculate nearest neighbors for (can adjust if needed, or leave the same)\n",
    "languages=['en','hi', 'bg', 'ar']\n",
    "\n",
    "# Precalculate nearest neighbors for each language\n",
    "for language in languages:\n",
    "    print(language)\n",
    "\n",
    "    # Precalculate nearest neighbors for each downsample\n",
    "    for seed in seeds:\n",
    "        print(seed)\n",
    "\n",
    "        print('Load model...')\n",
    "        with open(glove_path+language+'/downsampled_without_replacement_glove_'+str(seed)+'.txt','r',encoding='latin-1') as embeddingFile:\n",
    "            embeddings = [i[:-1].split(' ') for i in embeddingFile.readlines()]\n",
    "            embedding_words = [i[0] for i in embeddings]\n",
    "\n",
    "        xb = np.matrix([[float(j) for j in i[1:]] for i in embeddings],dtype='float32') #database\n",
    "\n",
    "        print('Normalizing vectors')\n",
    "        for i in trange(len(xb)):\n",
    "            xb[i] = normalize(xb[i])\n",
    "\n",
    "        d = xb.shape[1] #dimension\n",
    "        nb = xb.shape[0] #database size\n",
    "        nq = len(embedding_words) #num queries\n",
    "        print('d',d)\n",
    "        print('nb',nb)\n",
    "        print('nq',nq)\n",
    "\n",
    "        print('Creating query matrix...')\n",
    "        xq = xb[[i for i in range(len(embedding_words))],:]\n",
    "        print(xq.shape)\n",
    "\n",
    "        print('Building index...')\n",
    "        faiss_index = faiss.IndexFlatL2(d)\n",
    "        faiss_index.add(xb)\n",
    "\n",
    "        k = 11 #number of nearest neighbors\n",
    "\n",
    "        print('Calculating nearest neighbors...')\n",
    "        D, I = faiss_index.search(xq, k)\n",
    "\n",
    "        nearestNeighbors = {}\n",
    "        print('Recording nearest neighbors...')\n",
    "        for i in tqdm(range(len(embedding_words))):\n",
    "            word = embedding_words[i]\n",
    "            nearestNeighbors[word] = [embedding_words[j] for j in I[i]][1:]\n",
    "\n",
    "        #Save final\n",
    "        print('Saving nearest neighbors...')\n",
    "        with open(output_path+language+'/downsampled_without_replacement_glove_nearestNeighbors_'+str(seed)+'.pkl','wb') as pickleFile:\n",
    "            pickle.dump(nearestNeighbors,pickleFile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e636401",
   "metadata": {},
   "source": [
    "# Calculate Stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25e8b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate stability for each language in Wikipedia.\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.neighbors import BallTree\n",
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import sys\n",
    "from tqdm import tqdm,trange\n",
    "import pandas as pd\n",
    "\n",
    "# SET THESE VARIABLES\n",
    "\n",
    "# Location of the nearest neighbors for each word (created using precalculateNearestNeighbors_wikipedia.py)\n",
    "# Files should be stored in the format {output_path}{language}/downsampled_without_replacement_glove_nearestNeighbors_{downsample_index}.pkl, where the pickle file is a dictionary where the keys are words and the values are lists of ten nearest neighbors for each word.\n",
    "glove_path = '/Users/nehakardam/Project-CSE517/polyglot/'\n",
    "\n",
    "# Location where output stability will be stored\n",
    "# Files will be stored in the format {output_path}{language}/{language}_downsampled_without_replacement_glove_stability.csv, where the csv file has columns \"word\" and \"stability\", and the stability value is recorded for each word\n",
    "output_path = glove_path\n",
    "\n",
    "# Indices of downsamples to process (can adjust if needed, or leave the same)\n",
    "seeds = [0,1,2,3,4]\n",
    "\n",
    "# Languages to calculate stability for (can adjust if needed, or leave the same)\n",
    "languages=['en','hi', 'bg', 'ar']\n",
    "\n",
    "# Calculates the stability of a word in two sets of embedding spaces\n",
    "# Assumes that you've already calculated the most similar words for the word\n",
    "#\n",
    "# @param word\n",
    "#    The word to calculate stability for\n",
    "# @param similar1\n",
    "#    The list of nearest neighbors to word in the first set of embedding spaces\n",
    "#    len(similar1) = # of embedding spaces in the first set\n",
    "#    For each i, len(similar1[i]) = # of nearest neighbors to consider (same for each i)\n",
    "# @param similar2\n",
    "#    The list of nearest neighbors to word in the second set of embedding spaces\n",
    "# @param same\n",
    "#    Are the two lists of embedding spaces the same? (default = False)\n",
    "#\n",
    "# @returns a float, the average stability of the word across the two sets of spaces\n",
    "#\n",
    "def stability(word,similar1,similar2,same=False):\n",
    "    if same and len(similar1) == 1:\n",
    "        return len(similar1[0])\n",
    "\n",
    "    sets1 = [set(a) for a in similar1]\n",
    "    if not same:\n",
    "        sets2 = [set(b) for b in similar2]\n",
    "    else:\n",
    "        sets2 = sets1\n",
    "\n",
    "    avgOverlap = 0\n",
    "    for i in range(len(similar1)):\n",
    "        for j in range(len(similar2)):\n",
    "            if not same or (same and i!=j):\n",
    "                avgOverlap += len(sets1[i] & sets2[j])\n",
    "\n",
    "    if same:\n",
    "        avgOverlap /= (len(similar1)*len(similar2)-len(similar1))\n",
    "    else:\n",
    "        avgOverlap /= (len(similar1)*len(similar2))\n",
    "    return avgOverlap\n",
    "\n",
    "# Calculate stability for each language\n",
    "for language in languages:\n",
    "    print(language)\n",
    "\n",
    "    print('Reading ten nearest neighbors...')\n",
    "    nearest_neighbors = []\n",
    "    words = set()\n",
    "    for seed in seeds:\n",
    "        print(seed)\n",
    "        with open(glove_path+language+'/downsampled_without_replacement_glove_nearestNeighbors_'+str(seed)+'.pkl','rb') as pickleFile:\n",
    "            nearest_neighbors.append(pickle.load(pickleFile))\n",
    "            _words = set(nearest_neighbors[-1].keys())\n",
    "            if len(words)==0:\n",
    "                words = _words\n",
    "            else:\n",
    "                words = words.intersection(_words)\n",
    "    words = list(words)\n",
    "\n",
    "    print('Calculating stabilities...')\n",
    "    stabilities = []\n",
    "    for word in tqdm(words):\n",
    "        most_similar = []\n",
    "        for i in range(5):\n",
    "            most_similar.append(nearest_neighbors[i][word])\n",
    "        stabilities.append(stability(word,most_similar,most_similar,True))\n",
    "\n",
    "    print('Writing output file...')\n",
    "    df = pd.DataFrame(data={'word':words,'stability':stabilities})\n",
    "    df.to_csv(output_path+language+'/'+language+'_downsampled_without_replacement_glove_stability.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
