{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hindi Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the Hindi file into 5 text file\n",
    "\n",
    "with open(\"/Users/nehakardam/Project-CSE517/polyglot/hi/full.txt\",'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "with open(\"/Users/nehakardam/Project-CSE517/w2v_wiki_alldown/hi/downsampled_without_replacement_0.txt\",'w') as file:\n",
    "    ln = int(len(lines)/50)\n",
    "    for line in lines[:ln]:\n",
    "        file.write(line)\n",
    "\n",
    "with open(\"/Users/nehakardam/Project-CSE517/w2v_wiki_alldown/hi/downsampled_without_replacement_1.txt\",'w') as file:\n",
    "    ln = int(len(lines)/50)\n",
    "    for line in lines[ln+1:2*ln]:\n",
    "        file.write(line)\n",
    "        \n",
    "with open(\"/Users/nehakardam/Project-CSE517/w2v_wiki_alldown/hi/downsampled_without_replacement_2.txt\",'w') as file:\n",
    "    ln = int(len(lines)/50)\n",
    "    for line in lines[2*ln+1:3*ln]:\n",
    "        file.write(line)\n",
    "        \n",
    "with open(\"/Users/nehakardam/Project-CSE517/w2v_wiki_alldown/hi/downsampled_without_replacement_3.txt\",'w') as file:\n",
    "    ln = int(len(lines)/50)\n",
    "    for line in lines[3*ln+1:4*ln]:\n",
    "        file.write(line)\n",
    "        \n",
    "with open(\"/Users/nehakardam/Project-CSE517/w2v_wiki_alldown/hi/downsampled_without_replacement_4.txt\",'w') as file:\n",
    "    ln = int(len(lines)/50)\n",
    "    for line in lines[4*ln+1:]:\n",
    "        file.write(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "721"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"/Users/nehakardam/Project-CSE517/w2v_wiki_alldown/hi/downsampled_without_replacement_1.txt\",'r') as file:\n",
    "    lines = file.readlines()\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the English file into 5 text file\n",
    "\n",
    "with open(\"/Users/nehakardam/Project-CSE517/polyglot/en/full.txt\",'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "with open(\"/Users/nehakardam/Project-CSE517/w2v_wiki_alldown/en/downsampled_without_replacement_0.txt\",'w') as file:\n",
    "    ln = int(len(lines)/500)\n",
    "    for line in lines[:ln]:\n",
    "        file.write(line)\n",
    "\n",
    "with open(\"/Users/nehakardam/Project-CSE517/w2v_wiki_alldown/en/downsampled_without_replacement_1.txt\",'w') as file:\n",
    "    ln = int(len(lines)/500)\n",
    "    for line in lines[ln+1:2*ln]:\n",
    "        file.write(line)\n",
    "        \n",
    "with open(\"/Users/nehakardam/Project-CSE517/w2v_wiki_alldown/en/downsampled_without_replacement_2.txt\",'w') as file:\n",
    "    ln = int(len(lines)/500)\n",
    "    for line in lines[2*ln+1:3*ln]:\n",
    "        file.write(line)\n",
    "        \n",
    "with open(\"/Users/nehakardam/Project-CSE517/w2v_wiki_alldown/en/downsampled_without_replacement_3.txt\",'w') as file:\n",
    "    ln = int(len(lines)/500)\n",
    "    for line in lines[3*ln+1:4*ln]:\n",
    "        file.write(line)\n",
    "        \n",
    "with open(\"/Users/nehakardam/Project-CSE517/w2v_wiki_alldown/en/downsampled_without_replacement_4.txt\",'w') as file:\n",
    "    ln = int(len(lines)/500)\n",
    "    for line in lines[4*ln+1:]:\n",
    "        file.write(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "879"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"/Users/nehakardam/Project-CSE517/w2v_wiki_alldown/en/downsampled_without_replacement_2.txt\",'r') as file:\n",
    "    lines = file.readlines()\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arabic Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the Arabic file into 5 text file\n",
    "\n",
    "with open(\"/Users/nehakardam/Project-CSE517/polyglot/ar/full.txt\",'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "with open(\"/Users/nehakardam/Project-CSE517/w2v_wiki_alldown/ar/downsampled_without_replacement_0.txt\",'w') as file:\n",
    "    ln = int(len(lines)/50)\n",
    "    for line in lines[:ln]:\n",
    "        file.write(line)\n",
    "\n",
    "with open(\"/Users/nehakardam/Project-CSE517/w2v_wiki_alldown/ar/downsampled_without_replacement_1.txt\",'w') as file:\n",
    "    ln = int(len(lines)/50)\n",
    "    for line in lines[ln+1:2*ln]:\n",
    "        file.write(line)\n",
    "        \n",
    "with open(\"/Users/nehakardam/Project-CSE517/w2v_wiki_alldown/ar/downsampled_without_replacement_2.txt\",'w') as file:\n",
    "    ln = int(len(lines)/50)\n",
    "    for line in lines[2*ln+1:3*ln]:\n",
    "        file.write(line)\n",
    "        \n",
    "with open(\"/Users/nehakardam/Project-CSE517/w2v_wiki_alldown/ar/downsampled_without_replacement_3.txt\",'w') as file:\n",
    "    ln = int(len(lines)/50)\n",
    "    for line in lines[3*ln+1:4*ln]:\n",
    "        file.write(line)\n",
    "        \n",
    "with open(\"/Users/nehakardam/Project-CSE517/w2v_wiki_alldown/ar/downsampled_without_replacement_4.txt\",'w') as file:\n",
    "    ln = int(len(lines)/50)\n",
    "    for line in lines[4*ln+1:]:\n",
    "        file.write(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1471"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"/Users/nehakardam/Project-CSE517/w2v_wiki_alldown/ar/downsampled_without_replacement_1.txt\",'r') as file:\n",
    "    lines = file.readlines()\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bulgarian Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the Arabic file into 5 text file\n",
    "\n",
    "with open(\"/Users/nehakardam/Project-CSE517/polyglot/bg/full.txt\",'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "with open(\"/Users/nehakardam/Project-CSE517/w2v_wiki_alldown/bg/downsampled_without_replacement_0.txt\",'w') as file:\n",
    "    ln = int(len(lines)/50)\n",
    "    for line in lines[:ln]:\n",
    "        file.write(line)\n",
    "\n",
    "with open(\"/Users/nehakardam/Project-CSE517/w2v_wiki_alldown/bg/downsampled_without_replacement_1.txt\",'w') as file:\n",
    "    ln = int(len(lines)/50)\n",
    "    for line in lines[ln+1:2*ln]:\n",
    "        file.write(line)\n",
    "        \n",
    "with open(\"/Users/nehakardam/Project-CSE517/w2v_wiki_alldown/bg/downsampled_without_replacement_2.txt\",'w') as file:\n",
    "    ln = int(len(lines)/50)\n",
    "    for line in lines[2*ln+1:3*ln]:\n",
    "        file.write(line)\n",
    "        \n",
    "with open(\"/Users/nehakardam/Project-CSE517/w2v_wiki_alldown/bg/downsampled_without_replacement_3.txt\",'w') as file:\n",
    "    ln = int(len(lines)/50)\n",
    "    for line in lines[3*ln+1:4*ln]:\n",
    "        file.write(line)\n",
    "        \n",
    "with open(\"/Users/nehakardam/Project-CSE517/w2v_wiki_alldown/bg/downsampled_without_replacement_4.txt\",'w') as file:\n",
    "    ln = int(len(lines)/50)\n",
    "    for line in lines[4*ln+1:]:\n",
    "        file.write(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1403"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"/Users/nehakardam/Project-CSE517/w2v_wiki_alldown/bg/downsampled_without_replacement_1.txt\",'r') as file:\n",
    "    lines = file.readlines()\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading language file through the tensor for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-14 12:18:13.130216: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "da = tfds.load('wiki40b/el', split='train', as_supervised=False)\n",
    "da = tfds.as_numpy(da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-14 12:18:13.278459: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'\\n_START_ARTICLE_\\n2246 \\xce\\x9c\\xcf\\x80\\xcf\\x8c\\xce\\xbf\\xcf\\x85\\xce\\xb5\\xce\\xbb\\n_START_SECTION_\\n\\xce\\xa6\\xcf\\x85\\xcf\\x83\\xce\\xb9\\xce\\xba\\xce\\xac \\xcf\\x87\\xce\\xb1\\xcf\\x81\\xce\\xb1\\xce\\xba\\xcf\\x84\\xce\\xb7\\xcf\\x81\\xce\\xb9\\xcf\\x83\\xcf\\x84\\xce\\xb9\\xce\\xba\\xce\\xac\\n_START_PARAGRAPH_\\n\\xce\\x97 \\xce\\xbc\\xce\\xad\\xcf\\x83\\xce\\xb7 \\xce\\xb4\\xce\\xb9\\xce\\xac\\xce\\xbc\\xce\\xb5\\xcf\\x84\\xcf\\x81\\xce\\xbf\\xcf\\x82 \\xcf\\x84\\xce\\xbf\\xcf\\x85 \\xce\\x9c\\xcf\\x80\\xcf\\x8c\\xce\\xbf\\xcf\\x85\\xce\\xb5\\xce\\xbb \\xce\\xb5\\xce\\xba\\xcf\\x84\\xce\\xb9\\xce\\xbc\\xce\\xac\\xcf\\x84\\xce\\xb1\\xce\\xb9 \\xcf\\x83\\xce\\xb5 44,21 \\xcf\\x87\\xce\\xb9\\xce\\xbb\\xce\\xb9\\xcf\\x8c\\xce\\xbc\\xce\\xb5\\xcf\\x84\\xcf\\x81\\xce\\xb1, \\xce\\xb5\\xce\\xbd\\xcf\\x8e \\xcf\\x84\\xce\\xbf \\xce\\xb3\\xce\\xb5\\xcf\\x89\\xce\\xbc\\xce\\xb5\\xcf\\x84\\xcf\\x81\\xce\\xb9\\xce\\xba\\xcf\\x8c \\xce\\xac\\xce\\xbb\\xce\\xb2\\xce\\xb5\\xce\\xb4\\xcf\\x8c \\xcf\\x84\\xce\\xbf\\xcf\\x85 \\xce\\xb5\\xce\\xaf\\xce\\xbd\\xce\\xb1\\xce\\xb9 0,054. \\xce\\x9f \\xcf\\x86\\xce\\xb1\\xcf\\x83\\xce\\xbc\\xce\\xb1\\xcf\\x84\\xce\\xb9\\xce\\xba\\xcf\\x8c\\xcf\\x82 \\xcf\\x84\\xcf\\x8d\\xcf\\x80\\xce\\xbf\\xcf\\x82 \\xcf\\x84\\xce\\xbf\\xcf\\x85 \\xce\\xb5\\xce\\xaf\\xce\\xbd\\xce\\xb1\\xce\\xb9 D. \\xce\\x9f \\xce\\x9c\\xcf\\x80\\xcf\\x8c\\xce\\xbf\\xcf\\x85\\xce\\xb5\\xce\\xbb \\xcf\\x80\\xce\\xb5\\xcf\\x81\\xce\\xb9\\xcf\\x83\\xcf\\x84\\xcf\\x81\\xce\\xad\\xcf\\x86\\xce\\xb5\\xcf\\x84\\xce\\xb1\\xce\\xb9 \\xce\\xb3\\xcf\\x8d\\xcf\\x81\\xcf\\x89 \\xce\\xb1\\xcf\\x80\\xcf\\x8c \\xcf\\x84\\xce\\xbf\\xce\\xbd \\xce\\xb5\\xce\\xb1\\xcf\\x85\\xcf\\x84\\xcf\\x8c \\xcf\\x84\\xce\\xbf\\xcf\\x85 \\xce\\xbc\\xce\\xaf\\xce\\xb1 \\xcf\\x86\\xce\\xbf\\xcf\\x81\\xce\\xac \\xce\\xba\\xce\\xac\\xce\\xb8\\xce\\xb5 4 \\xcf\\x8e\\xcf\\x81\\xce\\xb5\\xcf\\x82 \\xce\\xba\\xce\\xb1\\xce\\xb9 59,5 \\xce\\xbb\\xce\\xb5\\xcf\\x80\\xcf\\x84\\xce\\xac.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = \"\"\n",
    "asciidata = \"\"\n",
    "for d in da:\n",
    "    data = d['text']\n",
    "    udata=data.decode(\"utf-8\")\n",
    "    asciidata=udata.encode(\"ascii\",\"ignore\")\n",
    "    break\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Greek, Modern\n",
    "el = tfds.load('wiki40b/el', split='train', as_supervised=False)\n",
    "el = tfds.as_numpy(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finnish\n",
    "fi = tfds.load('wiki40b/fi', split='train', as_supervised=False)\n",
    "fi = tfds.as_numpy(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Croatian\n",
    "hr = tfds.load('wiki40b/hr', split='train', as_supervised=False)\n",
    "hr = tfds.as_numpy(hr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estonian\n",
    "et = tfds.load('wiki40b/et', split='train', as_supervised=False)\n",
    "et = tfds.as_numpy(et)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lithuanian\n",
    "lt = tfds.load('wiki40b/lt', split='train', as_supervised=False)\n",
    "lt = tfds.as_numpy(lt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Latvian\n",
    "lv = tfds.load('wiki40b/lv', split='train', as_supervised=False)\n",
    "lv = tfds.as_numpy(lv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Malay\n",
    "ms = tfds.load('wiki40b/ms', split='train', as_supervised=False)\n",
    "ms = tfds.as_numpy(ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slovak\n",
    "sk = tfds.load('wiki40b/sk', split='train', as_supervised=False)\n",
    "sk = tfds.as_numpy(sk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slovene\n",
    "sl = tfds.load('wiki40b/sl', split='train', as_supervised=False)\n",
    "sl = tfds.as_numpy(sl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_IterableDataset' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5c/hs39rcwx6319812pwgk7r0w40000gn/T/ipykernel_4505/3008253930.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wiki40b/tl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_supervised\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: '_IterableDataset' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "#Tagalog\n",
    "tl = tfds.load('wiki40b/tl', split='train', as_supervised=False)\n",
    "tl = tfds.as_numpy(tl)\n",
    "tl.text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
